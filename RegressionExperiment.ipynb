{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "# define objective function\n",
    "def sigmoid_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# define calculate gradient function\n",
    "def calculate_gradient(given_x, given_y, omega):\n",
    "    hw = sigmoid_function(np.dot(given_x, omega))\n",
    "    a = hw - given_y\n",
    "    gradient_ = np.dot(given_x.T, a) / given_y.size\n",
    "    return gradient_\n",
    "\n",
    "\n",
    "# define loss function\n",
    "def loss_function(given_x, given_y, omega):\n",
    "    hw = sigmoid_function(np.dot(given_x, omega))\n",
    "    loss = -np.mean(given_y * np.log(hw) + (1 - given_y) * np.log(1 - hw))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# load training data set\n",
    "X_train, y_train = load_svmlight_file(\"a9a_train\")\n",
    "X_train = X_train.toarray()\n",
    "X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n",
    "y_train = y_train + np.ones(y_train.size)\n",
    "y_train = y_train / 2\n",
    "\n",
    "# load validation data set\n",
    "X_test, y_test = load_svmlight_file(\"a9a_test.t\")\n",
    "X_test = X_test.toarray()\n",
    "X_test = np.column_stack((X_test, np.zeros(X_test.shape[0])))\n",
    "X_test = np.column_stack((X_test, np.ones(X_test.shape[0])))\n",
    "y_test = y_test + np.ones(y_test.size)\n",
    "y_test = y_test / 2\n",
    "\n",
    "# parameters initialize - zeros\n",
    "SGD_theta = np.zeros(124)\n",
    "NAG_theta = np.zeros(124)\n",
    "RMSProp_theta = np.zeros(124)\n",
    "AdaDelta_theta = np.zeros(124)\n",
    "Adam_theta = np.zeros(124)\n",
    "\n",
    "# define learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# define iteration number\n",
    "iteration_num = 1000\n",
    "# define batch number\n",
    "batch_number = 100\n",
    "\n",
    "# loss\n",
    "SGD_loss = []\n",
    "NAG_loss = []\n",
    "RMSProp_loss = []\n",
    "Adadelta_loss = []\n",
    "Adam_loss = []\n",
    "# parameters for NAG\n",
    "Alpha=0.9\n",
    "v_theta=0\n",
    "# parameters for RMSPorp\n",
    "r_theta=0\n",
    "decay_rate=0.9\n",
    "delta=0.00000001\n",
    "# parameters for Adadelta\n",
    "o_theta=0\n",
    "s_theta=learning_rate\n",
    "Delta_theta=0\n",
    "# parameters for Adam\n",
    "decay_rate2=0.999\n",
    "rho_1=0\n",
    "rho_2=0\n",
    "for i in range(iteration_num):\n",
    "\n",
    "    index = random.randint(0, y_train.size - batch_number)\n",
    "\n",
    "    SGD_gradient = calculate_gradient(X_train[index:index + batch_number], y_train[index:index + batch_number], SGD_theta)\n",
    "\n",
    "    # update parameters\n",
    "    SGD_theta = SGD_theta - learning_rate * SGD_gradient\n",
    "\n",
    "    SGD_loss.append(loss_function(X_test, y_test, SGD_theta))\n",
    "\n",
    "    # NAG\n",
    "    NAG_gradient=calculate_gradient(X_train[index:index + batch_number], y_train[index:index + batch_number],NAG_theta+Alpha*v_theta)\n",
    "    v_theta=Alpha*v_theta- learning_rate * NAG_gradient\n",
    "    NAG_theta+=v_theta\n",
    "    NAG_loss.append(loss_function(X_test, y_test, NAG_theta))\n",
    "\n",
    "    # RMSProp\n",
    "    RMSProp_gradient=calculate_gradient(X_train[index:index + batch_number], y_train[index:index + batch_number],RMSProp_theta)\n",
    "    r_theta=decay_rate*r_theta+(1-decay_rate)*(RMSProp_gradient**2)\n",
    "    RMSProp_theta = RMSProp_theta-learning_rate*RMSProp_gradient/(np.sqrt(r_theta+delta))\n",
    "    RMSProp_loss.append(loss_function(X_test, y_test, RMSProp_theta))\n",
    "\n",
    "    # Adadelta\n",
    "    Adadelta_gradient=calculate_gradient(X_train[index:index + batch_number], y_train[index:index + batch_number],AdaDelta_theta)\n",
    "    o_theta = decay_rate * o_theta + (1 - decay_rate) * (Adadelta_gradient ** 2)\n",
    "    if i != 0:\n",
    "        s_theta = decay_rate * s_theta + (1 - decay_rate) * Delta_theta ** 2\n",
    "\n",
    "    Delta_theta = np.sqrt(s_theta) / (np.sqrt(o_theta + delta)) * Adadelta_gradient\n",
    "    AdaDelta_theta-=Delta_theta\n",
    "    Adadelta_loss.append(loss_function(X_test, y_test, AdaDelta_theta))\n",
    "\n",
    "    # Adam\n",
    "    Adam_gradient=calculate_gradient(X_train[index:index + batch_number], y_train[index:index + batch_number],Adam_theta)\n",
    "    rho_1=decay_rate*rho_1+(1-decay_rate)*Adam_gradient\n",
    "    rho_2=decay_rate2*rho_2+(1-decay_rate2)*(Adam_gradient**2)\n",
    "    rho_1_ = rho_1 / (1 - decay_rate ** (i + 1))\n",
    "    rho_2_ = rho_2/(1-decay_rate2**(i+1))\n",
    "    Adam_theta=Adam_theta-rho_1_*learning_rate/(np.sqrt(rho_2_+delta))\n",
    "    Adam_loss.append(loss_function(X_test, y_test, Adam_theta))\n",
    "# plot loss changes with iteration\n",
    "# yellow line for validation data loss\n",
    "plt.plot(Adam_loss,label=\"Adam\")\n",
    "plt.plot(Adadelta_loss,label=\"AdaDelta\")\n",
    "plt.plot(SGD_loss,label=\"SGD\")\n",
    "plt.plot(NAG_loss,label=\"NAG\")\n",
    "plt.plot(RMSProp_loss,label=\"RMSProp\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Linear Regression and gradient decent\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
